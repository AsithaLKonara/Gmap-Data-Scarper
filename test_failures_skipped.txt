tests/e2e/test_data_volume.py::TestDataVolume::test_error_recovery_scenarios FAILED [ 12%]
tests/e2e/test_deployment.py::TestAPIHealth::test_root_endpoint FAILED   [ 13%]
tests/e2e/test_deployment.py::TestAPIHealth::test_health_endpoint FAILED [ 13%]
tests/e2e/test_deployment.py::TestAPIHealth::test_metrics_endpoint FAILED [ 14%]
tests/e2e/test_deployment.py::TestScrapingWorkflow::test_start_scraping_task FAILED [ 14%]
tests/e2e/test_deployment.py::TestScrapingWorkflow::test_get_task_status FAILED [ 15%]
tests/e2e/test_deployment.py::TestScrapingWorkflow::test_stop_task FAILED [ 15%]
tests/e2e/test_deployment.py::TestScrapingWorkflow::test_list_tasks FAILED [ 16%]
tests/e2e/test_deployment.py::TestConcurrency::test_multiple_concurrent_tasks FAILED [ 17%]
tests/e2e/test_deployment.py::TestWebSocket::test_websocket_logs_connection SKIPPEDecause the target machine actively refused it) [ 17%]
tests/e2e/test_deployment.py::TestDataVolume::test_export_with_many_results FAILED [ 18%]
tests/e2e/test_deployment.py::TestErrorRecovery::test_invalid_task_id FAILED [ 18%]
tests/e2e/test_deployment.py::TestErrorRecovery::test_invalid_request_data FAILED [ 19%]
tests/e2e/test_scraping_flow.py::TestCompleteScrapingFlow::test_start_scrape_and_get_results FAILED [ 19%]
tests/e2e/test_scraping_flow.py::TestCompleteScrapingFlow::test_pause_and_resume_workflow FAILED [ 20%]
tests/e2e/test_scraping_flow.py::TestCompleteScrapingFlow::test_bulk_actions_workflow FAILED [ 20%]
tests/e2e/test_websocket_stability.py::TestWebSocketStability::test_websocket_logs_stream_stability FAILED [ 21%]
tests/e2e/test_websocket_stability.py::TestWebSocketStability::test_websocket_progress_stream FAILED [ 21%]
tests/e2e/test_websocket_stability.py::TestWebSocketStability::test_websocket_results_stream FAILED [ 22%]
tests/error_handling/test_network_errors.py::test_http_client_handles_timeout PASSED [ 25%]
tests/error_handling/test_network_errors.py::test_http_client_handles_connection_error PASSED [ 25%]
tests/error_handling/test_network_errors.py::test_http_client_retries_on_failure PASSED [ 26%]
tests/error_handling/test_network_errors.py::test_site_search_handles_http_errors PASSED [ 26%]
tests/integration/test_e2e.py::test_e2e_complete_scraping_session SKIPPEDesult: [Errno 13] Permission denied:
[ERROR] Failed to write result: [Errno 13] Permission denied:
[ERROR] Failed to write result: [Errno 13] Permission denied:
tests/integration/test_e2e.py::test_e2e_csv_output_format SKIPPED (CSV
tests/integration/test_orchestrator.py::test_orchestrator_runs_scrapers SKIPPEDite result: [Errno 13] Permission denied:
[ERROR] Failed to write result: [Errno 13] Permission denied:
tests/integration/test_orchestrator.py::test_orchestrator_with_google_maps SKIPPEDte issues) [ 34%]
tests/integration/test_orchestrator.py::test_orchestrator_multi_platform_session SKIPPEDte issues) [ 35%]
tests/ocr/test_image_phone_ocr.py::TestImagePhoneOCR::test_extract_text_from_image SKIPPED [ 47%]
tests/ocr/test_image_phone_ocr.py::TestImagePhoneOCR::test_extract_phone_from_image SKIPPED at position 149 (line 5, column 19)) [ 47%]
tests/performance/test_benchmarks.py::TestPerformanceBenchmarks::test_health_endpoint_performance SKIPPEDst', port=8000): Max retries exceeded
NewConnectionError('<urllib3.connection.HTTPConnection object at
0x00000175BCBCDEE0>: Failed to establish a new connection: [WinError
tests/performance/test_benchmarks.py::TestPerformanceBenchmarks::test_task_creation_performance SKIPPEDst', port=8000): Max retries exceeded
NewConnectionError('<urllib3.connection.HTTPConnection object at
0x00000175BCB84980>: Failed to establish a new connection: [WinError
tests/performance/test_benchmarks.py::TestPerformanceBenchmarks::test_concurrent_task_creation SKIPPED [ 50%]
tests/performance/test_benchmarks.py::TestPerformanceBenchmarks::test_list_tasks_performance SKIPPEDst', port=8000): Max retries exceeded
NewConnectionError('<urllib3.connection.HTTPConnection object at
0x00000175BC279070>: Failed to establish a new connection: [WinError
tests/platform/test_linkedin_scraper.py::test_linkedin_scraper_handles_extraction_errors PASSED [ 64%]
tests/test_comprehensive_api.py::TestScraperEndpoints::test_start_scraper_empty_queries FAILED [ 73%]
tests/test_comprehensive_api.py::TestScraperEndpoints::test_start_scraper_invalid_platform FAILED [ 74%]
tests/test_comprehensive_api.py::TestAIEndpoints::test_generate_queries FAILED [ 79%]
tests/test_comprehensive_api.py::TestSecurity::test_protected_endpoint_without_auth FAILED [ 81%]
tests/test_comprehensive_api.py::TestSecurity::test_protected_endpoint_with_invalid_token FAILED [ 81%]
tests/test_comprehensive_api.py::TestSecurity::test_sql_injection_prevention FAILED [ 82%]
tests/test_comprehensive_api.py::TestSecurity::test_xss_prevention FAILED [ 82%]
tests/test_new_endpoints.py::TestTeamsAPI::test_create_team FAILED       [ 83%]
tests/test_new_endpoints.py::TestTeamsAPI::test_list_teams FAILED        [ 84%]
tests/test_new_endpoints.py::TestAnalyticsAPI::test_dashboard_metrics FAILED [ 84%]
tests/test_new_endpoints.py::TestAnalyticsAPI::test_pipeline_metrics FAILED [ 85%]
tests/test_new_endpoints.py::TestAnalyticsAPI::test_forecast FAILED      [ 85%]
tests/test_new_endpoints.py::TestPredictiveAPI::test_conversion_prediction FAILED [ 86%]
tests/test_new_endpoints.py::TestPredictiveAPI::test_churn_prediction FAILED [ 86%]
tests/test_new_endpoints.py::TestPredictiveAPI::test_sentiment_analysis FAILED [ 87%]
tests/test_new_endpoints.py::TestPredictiveAPI::test_intent_detection FAILED [ 87%]
tests/test_new_endpoints.py::TestReportsAPI::test_build_report FAILED    [ 88%]
tests/test_new_endpoints.py::TestReportsAPI::test_create_scheduled_report FAILED [ 89%]
tests/test_new_endpoints.py::TestWorkflowsAPI::test_create_workflow FAILED [ 89%]
tests/test_new_endpoints.py::TestBrandingAPI::test_get_branding FAILED   [ 90%]
C:\Users\asith\Documents\gmap-data-scraper\gmap-data-scraper\backend\middleware\rate_limit.py:205: fastapi.exceptions.HTTPException: 429: {'error': 'Rate limit exceeded', 'message': 'Too many requests. Limit: 10 per 60 seconds', 'retry_after': 18}
C:\Users\asith\AppData\Local\Programs\Python\Python312\Lib\site-packages\requests\adapters.py:677: requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BC454C50>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
C:\Users\asith\AppData\Local\Programs\Python\Python312\Lib\site-packages\requests\adapters.py:677: requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/health (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BCC41730>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
C:\Users\asith\AppData\Local\Programs\Python\Python312\Lib\site-packages\requests\adapters.py:677: requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/health/metrics/prometheus (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BC4DF3E0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
C:\Users\asith\AppData\Local\Programs\Python\Python312\Lib\site-packages\requests\adapters.py:677: requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/scraper/start (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BC278B90>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
C:\Users\asith\AppData\Local\Programs\Python\Python312\Lib\site-packages\requests\adapters.py:677: requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/scraper/start (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BC4DE0F0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
C:\Users\asith\AppData\Local\Programs\Python\Python312\Lib\site-packages\requests\adapters.py:677: requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/scraper/start (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BC6CB3E0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
C:\Users\asith\AppData\Local\Programs\Python\Python312\Lib\site-packages\requests\adapters.py:677: requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/tasks (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BC279A00>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
C:\Users\asith\AppData\Local\Programs\Python\Python312\Lib\site-packages\requests\adapters.py:677: requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/scraper/start (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BCC415E0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
C:\Users\asith\AppData\Local\Programs\Python\Python312\Lib\site-packages\requests\adapters.py:677: requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/export/csv (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BC4DC8F0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
C:\Users\asith\AppData\Local\Programs\Python\Python312\Lib\site-packages\requests\adapters.py:677: requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/scraper/status/invalid-task-id (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BC27ABA0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
C:\Users\asith\AppData\Local\Programs\Python\Python312\Lib\site-packages\requests\adapters.py:677: requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/scraper/start (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BCC40830>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
C:\Users\asith\AppData\Local\Programs\Python\Python312\Lib\site-packages\requests\adapters.py:677: requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/scraper/start (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BCC419A0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
C:\Users\asith\AppData\Local\Programs\Python\Python312\Lib\site-packages\requests\adapters.py:677: requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/scraper/start (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BC2CF290>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
C:\Users\asith\AppData\Local\Programs\Python\Python312\Lib\site-packages\requests\adapters.py:677: requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/scraper/start (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BC278380>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
C:\Users\asith\AppData\Local\Programs\Python\Python312\Lib\asyncio\windows_events.py:599: ConnectionRefusedError: [WinError 1225] The remote computer refused the network connection
C:\Users\asith\AppData\Local\Programs\Python\Python312\Lib\asyncio\windows_events.py:599: ConnectionRefusedError: [WinError 1225] The remote computer refused the network connection
C:\Users\asith\AppData\Local\Programs\Python\Python312\Lib\asyncio\windows_events.py:599: ConnectionRefusedError: [WinError 1225] The remote computer refused the network connection
C:\Users\asith\Documents\gmap-data-scraper\gmap-data-scraper\backend\middleware\rate_limit.py:205: fastapi.exceptions.HTTPException: 429: {'error': 'Rate limit exceeded', 'message': 'Too many requests. Limit: 10 per 60 seconds', 'retry_after': 37}
C:\Users\asith\Documents\gmap-data-scraper\gmap-data-scraper\backend\middleware\rate_limit.py:205: fastapi.exceptions.HTTPException: 429: {'error': 'Rate limit exceeded', 'message': 'Too many requests. Limit: 10 per 60 seconds', 'retry_after': 37}
  C:\Users\asith\Documents\gmap-data-scraper\gmap-data-scraper\backend\models\schemas.py:10: PydanticDeprecatedSince20: `min_items` is deprecated and will be removed, use `min_length` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
  C:\Users\asith\Documents\gmap-data-scraper\gmap-data-scraper\backend\models\schemas.py:10: PydanticDeprecatedSince20: `max_items` is deprecated and will be removed, use `max_length` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
  C:\Users\asith\Documents\gmap-data-scraper\gmap-data-scraper\backend\models\schemas.py:11: PydanticDeprecatedSince20: `min_items` is deprecated and will be removed, use `min_length` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
  C:\Users\asith\Documents\gmap-data-scraper\gmap-data-scraper\backend\models\schemas.py:11: PydanticDeprecatedSince20: `max_items` is deprecated and will be removed, use `max_length` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
  C:\Users\asith\Documents\gmap-data-scraper\gmap-data-scraper\backend\models\schemas.py:30: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
  C:\Users\asith\Documents\gmap-data-scraper\gmap-data-scraper\backend\models\schemas.py:50: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
  C:\Users\asith\Documents\gmap-data-scraper\gmap-data-scraper\backend\models\schemas.py:65: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
  C:\Users\asith\Documents\gmap-data-scraper\gmap-data-scraper\backend\models\schemas.py:72: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
  C:\Users\asith\Documents\gmap-data-scraper\gmap-data-scraper\backend\models\schemas.py:79: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
  C:\Users\asith\Documents\gmap-data-scraper\gmap-data-scraper\backend\routes\workflows.py:15: PydanticDeprecatedSince20: `min_items` is deprecated and will be removed, use `min_length` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
  C:\Users\asith\Documents\gmap-data-scraper\gmap-data-scraper\backend\services\orchestrator_service.py:60: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
FAILED tests/e2e/test_data_volume.py::TestDataVolume::test_error_recovery_scenarios - fastapi.exceptions.HTTPException: 429: {'error': 'Rate limit exceeded', 'message': 'Too many requests. Limit: 10 per 60 seconds', 'retry_after': 18}
FAILED tests/e2e/test_deployment.py::TestAPIHealth::test_root_endpoint - requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BC454C50>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
FAILED tests/e2e/test_deployment.py::TestAPIHealth::test_health_endpoint - requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/health (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BCC41730>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
FAILED tests/e2e/test_deployment.py::TestAPIHealth::test_metrics_endpoint - requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/health/metrics/prometheus (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BC4DF3E0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
FAILED tests/e2e/test_deployment.py::TestScrapingWorkflow::test_start_scraping_task - requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/scraper/start (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BC278B90>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
FAILED tests/e2e/test_deployment.py::TestScrapingWorkflow::test_get_task_status - requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/scraper/start (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BC4DE0F0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
FAILED tests/e2e/test_deployment.py::TestScrapingWorkflow::test_stop_task - requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/scraper/start (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BC6CB3E0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
FAILED tests/e2e/test_deployment.py::TestScrapingWorkflow::test_list_tasks - requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/tasks (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BC279A00>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
FAILED tests/e2e/test_deployment.py::TestConcurrency::test_multiple_concurrent_tasks - requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/scraper/start (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BCC415E0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
FAILED tests/e2e/test_deployment.py::TestDataVolume::test_export_with_many_results - requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/export/csv (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BC4DC8F0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
FAILED tests/e2e/test_deployment.py::TestErrorRecovery::test_invalid_task_id - requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/scraper/status/invalid-task-id (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BC27ABA0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
FAILED tests/e2e/test_deployment.py::TestErrorRecovery::test_invalid_request_data - requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/scraper/start (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BCC40830>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
FAILED tests/e2e/test_scraping_flow.py::TestCompleteScrapingFlow::test_start_scrape_and_get_results - requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/scraper/start (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BCC419A0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
FAILED tests/e2e/test_scraping_flow.py::TestCompleteScrapingFlow::test_pause_and_resume_workflow - requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/scraper/start (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BC2CF290>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
FAILED tests/e2e/test_scraping_flow.py::TestCompleteScrapingFlow::test_bulk_actions_workflow - requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /api/scraper/start (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000175BC278380>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
FAILED tests/e2e/test_websocket_stability.py::TestWebSocketStability::test_websocket_logs_stream_stability - ConnectionRefusedError: [WinError 1225] The remote computer refused the network connection
FAILED tests/e2e/test_websocket_stability.py::TestWebSocketStability::test_websocket_progress_stream - ConnectionRefusedError: [WinError 1225] The remote computer refused the network connection
FAILED tests/e2e/test_websocket_stability.py::TestWebSocketStability::test_websocket_results_stream - ConnectionRefusedError: [WinError 1225] The remote computer refused the network connection
FAILED tests/test_comprehensive_api.py::TestScraperEndpoints::test_start_scraper_empty_queries - assert 422 == 400
FAILED tests/test_comprehensive_api.py::TestScraperEndpoints::test_start_scraper_invalid_platform - assert 422 in [200, 400]
FAILED tests/test_comprehensive_api.py::TestAIEndpoints::test_generate_queries - assert 404 in [200, 401, 403, 500]
FAILED tests/test_comprehensive_api.py::TestSecurity::test_protected_endpoint_without_auth - assert 404 == 401
FAILED tests/test_comprehensive_api.py::TestSecurity::test_protected_endpoint_with_invalid_token - assert 404 == 401
FAILED tests/test_comprehensive_api.py::TestSecurity::test_sql_injection_prevention - fastapi.exceptions.HTTPException: 429: {'error': 'Rate limit exceeded', 'message': 'Too many requests. Limit: 10 per 60 seconds', 'retry_after': 37}
FAILED tests/test_comprehensive_api.py::TestSecurity::test_xss_prevention - fastapi.exceptions.HTTPException: 429: {'error': 'Rate limit exceeded', 'message': 'Too many requests. Limit: 10 per 60 seconds', 'retry_after': 37}
FAILED tests/test_new_endpoints.py::TestTeamsAPI::test_create_team - assert 401 == 200
FAILED tests/test_new_endpoints.py::TestTeamsAPI::test_list_teams - assert 401 == 200
FAILED tests/test_new_endpoints.py::TestAnalyticsAPI::test_dashboard_metrics - assert 401 == 200
FAILED tests/test_new_endpoints.py::TestAnalyticsAPI::test_pipeline_metrics - assert 401 == 200
FAILED tests/test_new_endpoints.py::TestAnalyticsAPI::test_forecast - assert 401 == 200
FAILED tests/test_new_endpoints.py::TestPredictiveAPI::test_conversion_prediction - assert 401 == 200
FAILED tests/test_new_endpoints.py::TestPredictiveAPI::test_churn_prediction - assert 401 == 200
FAILED tests/test_new_endpoints.py::TestPredictiveAPI::test_sentiment_analysis - assert 401 == 200
FAILED tests/test_new_endpoints.py::TestPredictiveAPI::test_intent_detection - assert 401 == 200
FAILED tests/test_new_endpoints.py::TestReportsAPI::test_build_report - assert 401 == 200
FAILED tests/test_new_endpoints.py::TestReportsAPI::test_create_scheduled_report - assert 401 == 200
FAILED tests/test_new_endpoints.py::TestWorkflowsAPI::test_create_workflow - assert 401 == 200
FAILED tests/test_new_endpoints.py::TestBrandingAPI::test_get_branding - assert 401 == 200
==== 38 failed, 132 passed, 12 skipped, 94 warnings in 2184.21s (0:36:24) =====
